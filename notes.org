* Tender
** Design
   Always operate from a cache, with the ability to demand synchronization for
   critical resources.
   
   All functionality should be completely independent, specifying a contract that
   other programs could satisfy, if they desire.
   
** keel get (--watch)
   - [X] operates on a set of selfLink-like refs
   - [X] produces an initial list (maybe optional) of the current state of all
     resources in the set (turns out this is the default behavior of the kube api)
   - [X] emits change events as resources in the set are created/updated/deleted
     
** keel apply
   - operates on three lists of resources
     - start list  :: the list that was last applied
     - target list :: the list that is to be applied
     - live list   :: the list of resources, in their current state, that match
                      either the start list or target list

   - the order of the lists defines the order of application:
     - create in list-order of the target list
     - destroy in reverse-list-order of the start list

   - fails if a resource exists in the cluster, but not in the start list,
     unless the resource has been marked as annexable
   - in case of a non-annexable failure, users should resolve conflicts
     themselves by modifying the target list (perhaps using a three-way merge,
     or some other resolution strategy), and then mark the updated resource as
     annexable (<- use the resource version, rather than just a bool)

   - returns three lists:
      {:applied []
       :annexed []
       :removed []}

   - to implement rollback, users may pass the start list as the target list,
     adding any annexed resources from the previous invocation of
     `tender apply`.  NOTE: whatever mechanism is used to record state should not
     record restored-annexed resources.

** keel stable
   - NOTE: it can't really just take the target list.  what about resources that
     are transitioning to "gone"?  they need some kind of special handling.
   - operates on a target list plus the current state of all resources in the
     target list
   - checks if all target items in the current state snapshot list are stable
     with respect to the target list (right number of replicas, pods healthy,
     etc)
   - returns two lists: stable resources and unstable resources

** tender deploy
   TODO: you missed the whole thing about ordering templates, grouping them,
         saving state, so it's resumable, eventually timing out (maybe)
   
   - takes start and target state (gets start state from configmap by default)
   - calculates watch set
   - starts dingy get --watch
   - runs keel apply
   - until keel stable: wait for dingy get --watch change

** tender delete
   - exact same logic as tender deploy, except the target state is nil
   
** tender wait
   TODO: is this necessary as a separate command, if we make `tender deploy` and
         `tender delete` idempotent, and include a `--wait` flag?
   
   - operates on a target list (wrong: see note at tender stable)
   - starts tender watch and maintains a cache of all interesting (relevant to
     the target list) resources in the cluster
   - calls tender stable when the resources have changed (maybe with debounce or
     other strategies)
   - when all resources have been stable long enough (user-configurable, like
     liveness/readiness checks), stop with success
   - when resources haven't become stable before a timeout, die

** Questions
   How do we deal with CRDs that may not have been applied yet?  Vela will need
   to recognize urls whose API resource definitions are not yet present in the
   cluster.  The only way I can see is to examine all resources being applied,
   find CRDs and generate the definitions client-side.  Then, when watching:
   - try to watch the custom resources requested (which might fail)
   - if it fails, watch the CRDs themselves, until a necessary one is created
   - at that point, start to watch for custom resources of that kind
   
   Can/should tender understand the semantics of ConfigMaps/Secrets that are
   mounted vs read into environment variables?  Spinnaker takes the always-safe-
   always-annoying stance that these kinds of resources should always be
   versioned.  There are benefits to both kinds of semantics.  Maybe some Pods
   _prefer_ to watch the file sytem for changes.  This should be controlled by
   an annotation.
   
   Can this question be pushed to the templating level?  Tender (maybe?) would
   want to enforce that old versions of ConfigMaps/Secrets are eventually
   cleaned up.  But, the [... would be nice to know what I was going to write
   here.]
   
   How does rollback differ from deploy, exactly?  Users want to be able to say
   just `tender rollback` to go back one version.  But, what if they want to go
   back farther?  Where are historical manifest sets stored?  Can this be made
   generic?  If it is, will it be ugly to use?
   
   Can we atomically apply a heterogenous list of Kubernetes resources?  Is this
   even desirable?  Maybe we'll want to wait for stability as we go.
   
   Is this always right for `tender apply`?  Maybe there should be a choice
   between LIFO and FIFO.
   - the order of the lists defines the order of application:
     - create in list-order of the target list
     - destroy in reverse-list-order of the start list
     
  
  
* Notes
** Install
     # asdf
     brew install asdf
     # Add to your .bash_profile or equivalent:
     # . $(brew --prefix asdf)/asdf.sh

     # GraalVM
     asdf plugin-add graalvm
     asdf install graalvm 19.2.0.1 # or higher, if available

     # Go (for Kubernetes in Docker)
     brew install go
     go version # expect 1.13+

     # kind (Kubernetes in Docker)
     GO111MODULE="on" go get sigs.k8s.io/kind@v0.5.1
     kind create cluster
     export KUBECONFIG=$(kind get kubeconfig-path)


** Emacs/Cider
   setenv JAVA_HOME to $(asdf where graalvm)
   maybe add this to .dir-locals.el
